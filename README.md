[# ðŸ‚¡ Blackjack Reinforcementâ€‘Learning Agent

> **Beat the dealer â€” or at least try.**  
> A fullyâ€‘custom Blackjack engine wrapped in a Gymâ€‘style environment and trained with
> modern RL algorithms (PPO & DQN) using [Stableâ€‘Baselines 3](https://github.com/DLR-RM/stable-baselines3).
> The environment allows agents to learn blackjack strategies with optional card-counting signals and evaluates their performance comprehensively.

[![license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE) ![python](https://img.shields.io/badge/python-3.9%2B-blue.svg)

---

## Table of Contents
1. [Motivation](#motivation)
2. [Project Layout](#project-layout)
3. [Installation](#installation)
4. [Quick Start](#quick-start)
5. [Environment Design](#environment-design)
6. [Training Scripts](#training-scripts)
7. [Benchmark Results](#benchmark-results)
8. [Interpreting the Results](#interpreting-the-results)
9. [How It Works](#how-it-works)
10. [Future Work](#future-work)
11. [License & Attribution](#license--attribution)

---

## Motivation
Casino blackjack serves as an excellent benchmark for sequential decisionâ€‘making under uncertainty. This project provides a rulesâ€‘accurate, multiâ€‘deck blackjack simulator integrated with Gym and explores two canonical deep RL algorithms:

- **Deep Q-Networks (DQN)**
- **Proximal Policy Optimization (PPO)**

Agents are trained to:

- Select optimal tables using normalized *true count* signals
- Decide actions (**HIT / STAND / DOUBLE / SPLIT**)
- Manage bankroll effectively over extended sessions

The environment emphasizes **partial observability**, **delayed rewards**, and **illegal-action masking** for robust RL training.

This project implements a **twoâ€‘stage Markov Decision Process** for blackjack:

1. **Tableâ€‘selection phase** â€“ the agent chooses which table (shoe) to join, implicitly
   learning cardâ€‘counting via the shoeâ€™s _true count_ feature.
2. **Inâ€‘hand phase** â€“ classic blackjack actions: **hit Â· stand Â· double Â· split**.

A handcrafted game engine (`game.py`) guarantees determinism and separates game
logic from RL concerns.  The Gym wrapper (`casino_env.py`) exposes a **multiâ€‘input
observation space** combining balance, card counts, dealer upâ€‘card and an
actionâ€‘mask for legalityâ€‘aware exploration.

Training, evaluation and largeâ€‘scale Monteâ€‘Carlo benchmarking scripts are
included so you can reproduce every number in this README with a single GPU.

---

## Project Layout
```
.
â”œâ”€â”€ game.py            # Core blackjack engine (cards, shoe, dealer logic)
â”œâ”€â”€ casino_env.py      # Gym environment with observation/action spaces, masking, and rewards
â”œâ”€â”€ train.py           # Training pipeline for PPO & DQN, TensorBoard logging
â”œâ”€â”€ evaluate.py        # Detailed singleâ€‘episode evaluation
â”œâ”€â”€ benchmark.py       # Parallel Monteâ€‘Carlo evaluation (100 k hands)
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md
```

---

## Installation
```bash
git clone https://github.com/amirmasoudaz/blackjack-rl.git
cd blackjack-rl
python -m venv .venv && source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

**Dependencies:**
* `stableâ€‘baselines3>=2.3`
* `gymnasium` (Gym compatibility layer shipped with SB3)
* `torch` (GPU if available)
* `tqdm`, `joblib`, `numpy`

---

## Quick Start
### Training agents
```bash
# PPO (default 1 M timesteps â‰ˆ 20 min on RTX 4060)
python train.py --model PPO
# DQN
python train.py --model DQN
```
TensorBoard logs are written to `tensorboard/{ppo|dqn}`.

### Evaluate trained agents
```bash
python evaluate.py --model_type PPO  # or DQN
```

### Run Monte-Carlo benchmark (100,000 hands, takes ~3min on 32 CPU cores)
```bash
python benchmark.py --model_type PPO    # or DQN
```

---

## Environment Design

| Element              | Description                                                                                      |
|----------------------|--------------------------------------------------------------------------------------------------|
| **Decks & Shuffle**  | 8 decks, reshuffle after 45â€“55 % of shoe played                                                  |
| **Rules**            | Dealer hits soft 17, no surrender or splits, DOUBLE allowed                                      |
| **Betting**          | Fixed bet with optional DOUBLE                                                                   |
| **Counting**         | Hi-Lo true count as normalized input                                                             |
| **Observation**      | `Dict(phase, balance, table_info, in_hand, true_count, action_mask)`                             |
| **Reward Structure** | +1 (win), 0 (push), -1 (loss), +1.5 (blackjack), step/illegal-action penalties                   |
| **Actions**          | Table selection, HIT, STAND, DOUBLE, SPLIT (masked for legality)                                 |

---

## Training Scripts

- **`train.py`**
  - Parallel environments (`SubprocVecEnv`, 32 copies)
  - TensorBoard integration for monitoring and logging
  - CUDA automatic detection and optimization

- **Agent configurations** ensure comparable training conditions for PPO and DQN, highlighting algorithmic differences clearly.

---

## Benchmark Results

| Metric (100 k hands) |  **DQN**   |   **PPO**   |
|----------------------|:----------:|:-----------:|
| Win rate             |   40.96%   | **42.02%**  |
| Push rate            | **9.62%**  |    7.69%    |
| Loss rate            | **49.42%** |   50.30%    |
| Blackjack rate       | **4.52%**  |    4.49%    |
| Avg reward / hand    |   âˆ’0.070   | **âˆ’0.060**  |

---

## Interpreting the Results
- Both agents maintain negative expected values due to inherent casino odds (~0.6% house edge).
- PPO slightly outperforms DQN in terms of win rate (by <2%) and average returns (1 cents per dollar), highlighting PPO's efficiency. Hyperâ€‘parameter tuning or Rainbowâ€‘style improvements may close the gap.
- Illegal-action masking significantly improves learning stability and reduces unnecessary exploration.
- Stepâ€‘wise illegalâ€‘action penalties speed up learning stability in DQN.
- Future enhancements in bet sizing and card-counting could further improve agent profitability.

---

## How It Works
### Environment Design
* **Observation Space** (multiâ€‘input dict)
  * *phase* âˆˆ {0,1}
  * *balance* âˆˆ [0,1]
  * *table_info* (shape =`(N_tables,4)`) â€“ min bet, alwaysâ€‘1 placeholder, clipped
    trueâ€‘count, occupancy flag
  * *in_hand* (shape =`(6,)`) â€“ player total, softness flag, dealer upâ€‘card, canâ€‘double,
    pair flag, card count
  * *true_count* â€“ scalar shoe count reused by counting agents
  * *action_mask* â€“ dynamic legality mask concatenated to the observation

* **Reward Shaping**
  * Win = +1, Push = 0, Loss = âˆ’1, Blackjack = +1.5
  * Small stepâ€‘penalty (0) keeps Qâ€‘values centered while allowing for additional
    shaping experiments.

### Training Details
* **PPO** â€“ 256â€‘step rollouts, 10 epochs, `clip=0.2`, `ent_coef=0.01`, Î³=0.99
* **DQN** â€“ 100k replay buffer, `train_freq=1`, Îµâ€‘greedy decay 0.5â†’0.05, target
  network update every 10k steps
* **Vectorised Envs** â€“ 32 parallel workers speed up sample collection Ã—30.

### Logging & Analysis
* **TensorBoard** â€“ automatic scalar & histogram logging per algorithm
* **LoggingWrapper** â€“ prints every decision with card details for debugging
* **Benchmark** â€“ embarrassingly parallel Monteâ€‘Carlo evaluator using `joblib`

---

## Future Work
- [ ] Multiâ€‘agent training (cooperative vs. competitive)
- [ ] Implement a more complex reward structure (e.g., risk-adjusted returns)
- [ ] Enable splits, insurance, and multi-hand strategies
- [ ] Implement advanced RL algorithms (Rainbow DQN and Distributional RL)
- [ ] Introduce variable betting and bankroll management
- [ ] Curriculum learning based on deck penetration and complexity

---

## License & Attribution

This project is released under the **MIT License**. See the [LICENSE](LICENSE) file for details.

If you find this project useful or build something on top of it, feel free to mention or link back to this repo â€” it'd mean a lot!

---]()